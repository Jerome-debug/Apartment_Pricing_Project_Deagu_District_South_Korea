---
title: "Group3_Project"
author: "Bundi Kirimi"
date: "2/7/2022"
output: html_document
---


1.  Defining the Question

<!-- -->

a)  Specifying the Data Analytic Question

 find out the factors that influence pricing of apartments

b)  Defining the Metric for Success 
    This project will be successful when our model reaches an Accuracy of between 75-95%

c)  Understanding the context
The dataset was acquired fro  the Deagu District in Korea.It outlines the various factors that influence the price of an Apartment.

d)Recording the Experimental Design
    Loading Libraries
    Reading the dataset
    Data Cleaning
    Exploratory Data Anlysis
    Feature Engineering
    Model Training
    Model Evaluation
    Conclusion
    Recommendations

e)  Data Relevance

<!-- -->

2.  Reading the Data

#Loading the libraries
````{r}
#specify the path where the file is located
library("data.table")
library(tidyverse)
library(magrittr)
library(warn = -1)
library("ggbiplot")
library(ggplot2)
library(lattice)
library(corrplot)
library(DataExplorer)
library(pastecs)
library(psych)
library(factoextra)
library(caret)

````


#Loading the data
```{r}
#Loading the data
path <-"C:/Users/Jerome/Downloads/Daegu_Real_Estate_data.csv" 

df<-read.csv(path, sep = ",")

head(df)
```


#Previewing the summary of the dataset
```{r}
summary(df)
```
The mean of Sale Price of an apartment in Deagu is 221218 whereas the median is 207964.
This suggests that the data is right_skewed/Positvely_skewed.

3)Checking the data
```{r}
#Length
length(df)
```
The dataframe has 30 columns

````{r}
#Dimensions
dim(df)

````

#The dataframe has 5891 row entries and 30 columns

Column Names
```{r}
colnames(df)

```

Column data types

```{r}
sapply(df, class)
```


Our data has numerical,categorical and character variables

4)Data Cleaning

```{r}
sum(is.na(df))

```
There are no Null Values



Checking for Dupicates
```{r}
#Duplicates
duplicated_rows <- df[duplicated(df),]

dim(duplicated_rows)

```
We found 316 rows weere duplicated


Removing duplicates
```{r}
df1 <- df[-which(duplicated(df)), ]
dim(df1)
```
Our data has 5575 rows and 30 columns after removing the duplicates


#Data Types
```{r}

sapply(df1, class)
```

```{r}
summary(df1)
```

train-rmse:32371.458984 
[2]	train-rmse:26778.394531 



#Univariate Analysis
Checking the categorical variables
```{r}
#Visualizing the Hallway types
unique(df1$HallwayType)

factor(unique(df1$HallwayType))

```


```{r}
#Visualizing the Heating type
unique(df1$HeatingType)

factor(unique(df1$HeatingType))
```

```{r}
#Visualizing the Apartment Type
unique(df1$AptManageType)

factor(unique(df1$AptManageType))
```



```{r}
#Visualizing the Hallway types

library(ggplot2)
ggplot(df1, aes(HallwayType)) + 
   geom_bar(fill = "red")
```

We can see most apartments have terraced Hallway types.

```{r}
#Visualizing the Apartment Management types

library(ggplot2)
ggplot(df1, aes(AptManageType)) + 
   geom_bar(fill = "Orange")
```

Most Apartments had  Management System  by trust while very few had self management


```{r}
#Visualizing the heating systems

library(ggplot2)
ggplot(df1, aes(HeatingType)) + 
   geom_bar(fill = "Blue")
```
Most Apartments had Individual Heating System while very few had central heating sytsems

```{r}
#
#each of the values printed below appear thrice in the dataset
#distribution
hist(df1$SalePrice , col=c("darkorange"))


```
We can witness a normal Distribution

```{r}
head(df1)
```
```{r}
summary(df1)
```

```{r}
head(df1)
```




```{r}
#Histogram of size in squarefoot
hist(df1$Size.sqf. , col=c("darkorange"))
```
We can witness a normal distribution



```{r}
#Histogram of floor column
#distribution
hist(df1$Floor , col=c("darkorange"))
```
The data is skewed to the right.
Thus suggests that there is very minimal buildings with so many floors compared to the ones with few floors

`


```{r}
df1$Age <- df1$YrSold - df1$YearBuilt
df1

```


```{r}


data <- df1[ c(1,5,6,7,8,9,10,11,14,15,16,18,19,20,21,22,23,28,29,30,31) ]

head(data)
```


```{r}
#Distrbution of difference in years of the year built and year sold

#distribution
hist(data$Age , col=c("darkorange"))

```
The data is skewed to the right.
Most houses in the dataset were newly built


```{r}
summary(data)
````



Bivariate Analysis
```{r}
head(data)
```
Sale Price Vs Size in Square foot of the house
```{r}
#Sale Price Vs Size in Square foot of the house
ggplot(data, aes(x = Floor, y = SalePrice)) +
  geom_point() +
  stat_smooth(method = 'lm')
```
It is likely the higher the Apartment in terms of floor 
the more price of the Apartment

```{r}
ggplot(data, aes(x = AptManageType, y = SalePrice)) +
  geom_boxplot() 
```
The Apartments managed by Trust are more expensive to Apartments managed by Individuals.


```{r}

ggplot(data, aes(x = Age, y = SalePrice)) +
  geom_point() +
  stat_smooth(method = 'lm')

```
As an Apartment Ages,the price Reduces

```{r}

              
    #create scatterplot of hours studied vs. exam score
plot(data$Size.sqf., data$SalePrice, pch=16, col='black',
     main='Size.sqf vs  SalePrice',
     xlab='Size.sqf', ylab='SalePrice ')


```
The Price of an Apartment increases with increase in Size of the Apartment


```{r}
#create scatterplot of hours studied vs. exam score
plot(data$SalePrice, data$N_SchoolNearBy.Total.
, pch=16, col='steelblue',
     main='SalePrice N_FacilitiesNearBy.Total.t',
     xlab='SalePrice', ylab='N_FacilitiesNearBy.Total. ')
```

There is no direct relationship between the two variables




```{r}
head(data)
```


Feature engineering

```{r}

#select list of categorical variables,df1
df2 <- data[,c(4,5,6)]#categorical
#make a subset of non-categorical variables,df2
df3 <-data[,c(1,2,3,7,8,9,10,11,15,18,19,20,21)]
```


```{r}
head(df3)
```


Dummy Encoding Categorical Variables

```{r}
dmy <- dummyVars(" ~ .", data = df2, fullRank = T)
df4 <- data.frame(predict(dmy, newdata = df2))

head(df4)
```
Joining the Categorical columns  and numerical columns 
````{r}
db <- cbind(df4,df3)
```

```{r}
head(db)
```


Checking for Dupicates
```{r}
#Duplicates
duplicated_rows <- db[duplicated(db),]

dim(duplicated_rows)

```



Removing duplicates
```{r}
df5 <- db[-which(duplicated(db)), ]
dim(df5)
```



Performing Scaling

```{r}
#dummy_df2_scaled <- scale(db)
#summary(dummy_df2_scaled)
```
Performing Normalization
```{r}
dummy_df2_norm <- as.data.frame(apply(df5, 2, function(x) (x -
min(x))/(max(x)-min(x))))
summary(dummy_df2_norm)
```


Correlation

```{r,fig.width=10,fig.height=10}
# Plotting the Correlation Heatmap
library(ggcorrplot)
ggcorrplot(cor(dummy_df2_norm), hc.order = F,type = 
"upper", lab = T ,
  ggtheme = ggplot2::theme_gray,
  colors = c("#00798c", "white", "#edae49"),
  tl.srt = 90)
```




##Creating train test sets
```{r}
library(caret)
set.seed(387)
train_data = createDataPartition(y = dummy_df2_norm$SalePrice, p = 0.70, list = FALSE)
test_data =createDataPartition(y=dummy_df2_norm$SalePrice, p=0.30,list=FALSE)
train = dummy_df2_norm[train_data, ]
test = dummy_df2_norm[test_data, ]
```
Feature Selection
```{r}

library(Boruta)
# Perform Boruta search
boruta_output <- Boruta(SalePrice ~ ., data=na.omit(train), doTrace=0)  

names(boruta_output)
```


```{r}
#install.packages("Boruta")
library(Boruta)


# Do a tentative rough fix
roughFixMod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(roughFixMod)
print(boruta_signif)
```
```{r}
# Variable Importance Scores
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2[order(-imps2$meanImp), ])  # descending sort
```


```{r, fig.height=10,fig.width=10}
# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  
```


#Moodel building
#linear regression
```{r}
model = lm( SalePrice ~., data = train)
```

##summary of the model
```{r}
summary(model) # Obtain coefficients, Residuals and statistics
rsquare = summary(model)$r.squared # R-squared value
```
Our model performs at 78.8%



##Predictions
````{r}
library(ggplot2)
predictions = predict(model, newdata = test)
predicted.vs.original = data.frame(predicted = predictions, original = test$SalePrice)   # Create a new data frame
ggplot(predicted.vs.original, aes(x = predicted, y = original)) +
 geom_point() +
 geom_smooth(color='blue') +
 labs(x = 'Predicted Values', y = 'Original Values', title = 'Predicted vs. Original Values') +
 theme_minimal()
````
The predicted values align themselves on the line of best fit hence meaning that Model works well
X G Boos Model
```{r}
library(xgboost)

bstSparse <- xgboost(data=as.matrix(train), label = train$SalePrice, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror")

```


```{r}
pred <- predict(bstSparse, newdata=as.matrix(test))

# size of the prediction vector
print(length(pred))
## [1] 1611

# limit display of predictions to the first few
print(head(pred))
```

```{r}
head(data)
```

#Polynomial regression
```{r}
 poly_reg = lm(formula = SalePrice ~ .,data = train)
```

```{r}
#summary(poly_reg)
summary(poly_reg)$r.squared
```
Our  Model has an Accuracy of 78% 


```{r}

predict = predict(poly_reg, newdata = test)
predicted.vs.original = data.frame(predicted = predict, original = test$SalePrice)   # Create a new data frame
ggplot(predicted.vs.original, aes(x = predicted, y = original)) +
 geom_point() +
 geom_smooth(color='blue') +
 labs(x = 'Predicted Values', y = 'Original Values', title = 'Predicted vs. Original Values') +
 theme_minimal()
```
K-MEANS CLUSTERING

```{r}
outputk <- kmeans(dummy_df2_norm, 4)
```

```{r}
outputk$size

outputk$centers

```
Determining Number of clusters using Elbow Mmethod
```{r}

library(factoextra)
# Elbow method
fviz_nbclust(dummy_df2_norm, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```
The number of clusters are 4,hence we are going to work with four clusters

Visualising the clusters

```{r}
library(factoextra)
options(repr.plot.width = 11, repr.plot.height = 10)
fviz_cluster(outputk, dummy_df2_norm)
```


HIERACHICAL CLUSTERING
```{r}

d <- dist(dummy_df2_norm, method = "euclidean")
# We then apply hierarchical clustering using the Ward's method
res.hc <- hclust(d, method = "ward.D2")
# Lastly we plot the obtained dendrogram
#--
plot(res.hc, cex = 0.6, hang = -1)

```
















