---
title: "Group3_Project"
author: "Bundi Kirimi"
date: "2/7/2022"
output: html_document
---
#Loading the libraries
````{r}
#specify the path where the file is located
library("data.table")
library(tidyverse)
library(magrittr)
library(warn = -1)
library("ggbiplot")
library(ggplot2)
library(lattice)
library(corrplot)
library(DataExplorer)
library(pastecs)
library(psych)
library(factoextra)
library(caret)

````



```{r}
#Loading the data
path <-"C:/Users/Jerome/Downloads/Daegu_Real_Estate_data.csv" 

df<-read.csv(path, sep = ",")

head(df)
```


#Previewing the summary of the dataset
```{r}
summary(df)
```

#Checking the data
```{r}
#Length
length(df)
```
The dataframe has 16 columns


````{r}
#Dimensions
dim(df)

````




#The dataframe has 12330 row entries and 18 columns
Column Names
```{r}
colnames(df)

```

Column data types

```{r}
sapply(df, class)
```




#Data Cleaning

```{r}

any(is.na(df))
```

```{r}
#Duplicates
duplicated_rows <- df[duplicated(df),]

dim(duplicated_rows)

```
Removing duplicates
```{r}
df1 <- df[-which(duplicated(df)), ]
dim(df1)
```

#Data Types
```{r}

sapply(df1, class)
```

```{r}
summary(df1)
```

median is 212,389..This will be the nedium priced house
32,743 will be the least price
585,840 the most expensive house

#Univariate Analysis

```{r}
unique(df1$HallwayType)

factor(unique(df1$HallwayType))

```


```{r}
unique(df1$HeatingType)

factor(unique(df1$HeatingType))
```

```{r}
unique(df1$AptManageType)

factor(unique(df1$AptManageType))
```



```{r}
#Visualizing the Hallway types

library(ggplot2)
ggplot(df1, aes(HallwayType)) + 
   geom_bar(fill = "red")
```

We can see most apartments had terraced Hallway types.

```{r}
#Visualizing the Apartment Management types

library(ggplot2)
ggplot(df1, aes(AptManageType)) + 
   geom_bar(fill = "Pink")
```

Most Apartments had some sort of Management while very few had self management


```{r}
#Visualizing the heating systems

library(ggplot2)
ggplot(df1, aes(HeatingType)) + 
   geom_bar(fill = "Blue")
```
Most Apartments had Individual Heating System

```{r}
#
#each of the values printed below appear thrice in the dataset
#distribution
hist(df1$SalePrice , col=c("darkorange"))


```
```{r}
#HallwayTypecorridor                   HallwayTypemixed                HallwayTypeterraced
#each of the values printed below appear thrice in the dataset
#distribution
hist(df1$Age , col=c("darkorange"))
````

```{r}
#HallwayTypecorridor                   HallwayTypemixed                HallwayTypeterraced
#each of the values printed below appear thrice in the dataset
#distribution
hist(df1$YrSold , col=c("darkorange"))

```
```{r}
#Histogram of size in squarefoot
hist(df1$Size.sqf. , col=c("darkorange"))
```

```{r}
#Histogram of floor column
#distribution
hist(df1$Floor , col=c("darkorange"))
```

```{r}
summary(df1)
```



```{r}
head(df1)
```

```{r}
df1$Age <- df1$YrSold - df1$YearBuilt
df1

```


```{r}


data <- df1[ c(1,5,6,7,8,9,10,11,14,15,16,18,19,20,21,22,23,28,29,30,31) ]

head(data)
```

Bivariate Analysis

```{r}

```



```{r}
head(df1)
```
Sale Price Vs Size in Square foot of the house
```{r}
#Sale Price Vs Size in Square foot of the house
ggplot(df1, aes(x = Floor, y = SalePrice)) +
  geom_point() +
  stat_smooth(method = 'lm')
```


```{r}
ggplot(df1, aes(x = AptManageType, y = SalePrice)) +
  geom_boxplot() 
```


```{r}
# Assign plot to a variable
surveys_plot <- ggplot(data = df1,
                       mapping = aes(x = SalePrice, y = Floor))

# Draw the plot
surveys_plot +
    geom_point()

```

```{r}
# Assign plot to a variable
surveys_plot <- ggplot(data = df1,
                       mapping = aes(x = Size.sqf., y = SalePrice))

# Draw the plot
surveys_plot +
    geom_point()

````

```{r}
#create scatterplot of hours studied vs. exam score
plot(df1$N_elevators, df1$N_Parkinglot.Basement.
, pch=16, col='steelblue',
     main='Elevator Parkinglot basement',
     xlab='Elevator', ylab='Basement ')
```
```{r}
head(df1)
```




```{r}

#select list of categorical variables,df1
df2 <- df1[,c(7,8,9,12,13,17)]#categorical
#make a subset of non-categorical variables,df2
data <-df1[,c(1,5,6,10,11,14,15,23,29,30)]
```


```{r}
head(data)
```




```{r}
dmy <- dummyVars(" ~ .", data = data, fullRank = T)
df3 <- data.frame(predict(dmy, newdata = data))
```

````{r}
head(df3)
```

````{r}
db <- cbind(df3,data)
```

```{r}
head(db)
```




```{r}
# Calculating the correlation matrix
# ---
#
correlationMatrix <- cor(db)
# Find attributes that are highly correlated
# ---
#
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# Highly correlated attributes
# ---
# 
highlyCorrelated
names(db[,highlyCorrelated])
```
Scaling


```{r}
#dummy_df2_scaled <- scale(db)
#summary(dummy_df2_scaled)
```
Normalization
```{r}
dummy_df2_norm <- as.data.frame(apply(db, 2, function(x) (x -
min(x))/(max(x)-min(x))))
summary(dummy_df2_norm)
```

```{r,fig.width=10,fig.height=10}
# Plotting the Correlation Heatmap
library(ggcorrplot)
ggcorrplot(cor(dummy_df2_norm), hc.order = F,type = 
"upper", lab = T ,
  ggtheme = ggplot2::theme_gray,
  colors = c("#00798c", "white", "#edae49"),
  tl.srt = 90)
```
```{r}

```

```{r}
#Duplicates
duplicated_rows <- dummy_df2_norm[duplicated(dummy_df2_norm),]

dim(duplicated_rows)

```
Removing duplicates
```{r}
data <- df[-which(duplicated(dummy_df2_norm)), ]
dim(data)
```

##creating train test sets
```{r}
set.seed(387)
train_data = createDataPartition(y = data$SalePrice, p = 0.70, list = FALSE)
test_data =createDataPartition(y=data$SalePrice, p=0.30,list=FALSE)
train = data[train_data, ]
test = data[test_data, ]
```


#model building
#linear regression
```{r}
model = lm( SalePrice ~., data = train)
```

##summary of the model
```{r}
summary(model) # Obtain coefficients, Residuals and statistics
rsquare = summary(model)$r.squared # R-squared value
```
##predictions
````{r}
predictions = predict(model, newdata = test)
predicted.vs.original = data.frame(predicted = predictions, original = test$SalePrice)   # Create a new data frame
ggplot(predicted.vs.original, aes(x = predicted, y = original)) +
 geom_point() +
 geom_smooth(color='blue') +
 labs(x = 'Predicted Values', y = 'Original Values', title = 'Predicted vs. Original Values') +
 theme_minimal()
````


````{r}
#install.packages('xgboost')
library(xgboost)

library(caret)
set.seed(1234)

cvcontrol <- trainControl(method = "repeatedcv",
                          number = 5,
                          repeats = 2,
                          allowParallel = TRUE)
set.seed(1978)
boosting1<- train(SalePrice ~., data = test,
                  method = "xgbTree",
                  trControl = cvcontrol,
                  tuneGrid = expand.grid(nrounds = 720,
                                         max_depth = 7,
                                         eta = 0.25,
                                         gamma = 2.2,
                                         colsample_bytree =1,
                                         min_child_weight = 1,
                                         subsample = 1))

`````

```{r}
summary((boosting1))
```
```{r}
bstSparse <- xgboost(data=as.matrix(train), label = train$SalePrice, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
```





```{R}
bstSparse <- xgboost(data=as.matrix(train), label = train$SalePrice, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror")
  
```


```{r}
pred <- predict(bstSparse, newdata=as.matrix(test))

# size of the prediction vector
print(length(pred))
## [1] 1611

# limit display of predictions to the first few
print(head(pred))
```
K-MEANS CLUSTERING


```{r}
outputk <- kmeans(data, 4)
```

```{r}
outputk$size

outputk$centers

```
Determining Number of clusters using Elbow Mmethod
```{r}
# Elbow method
fviz_nbclust(data, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```


Visualising the clusters

```{r}
library(factoextra)
options(repr.plot.width = 11, repr.plot.height = 10)
fviz_cluster(outputk, data)
```


HIERACHICAL CLUSTERING
```{r}

d <- dist(data, method = "euclidean")
# We then apply hierarchical clustering using the Ward's method
res.hc <- hclust(d, method = "ward.D2")
# Lastly we plot the obtained dendrogram
#--
plot(res.hc, cex = 0.6, hang = -1)

```


```{r}
library(xgboost)
library(caret)


```



```{r}
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the data
data(dummy_df)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(dummy_df[,1:8], dummy_df[,9], sizes=c(1:8), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```


Pair Plots

```{r}
pairs(dummy_df[,1:10])


```









Regression
```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(dummy_df), replace = T, prob = c(0.6,0.4))
train <- advertising[sample, ]
test <- advertising[!sample, ]
```



